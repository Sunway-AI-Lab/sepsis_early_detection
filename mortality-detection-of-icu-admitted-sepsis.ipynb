{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff59548",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:06.120546Z",
     "iopub.status.busy": "2024-12-09T23:21:06.119993Z",
     "iopub.status.idle": "2024-12-09T23:21:07.306109Z",
     "shell.execute_reply": "2024-12-09T23:21:07.304804Z"
    },
    "papermill": {
     "duration": 1.195547,
     "end_time": "2024-12-09T23:21:07.309162",
     "exception": false,
     "start_time": "2024-12-09T23:21:06.113615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae512d6",
   "metadata": {
    "papermill": {
     "duration": 0.003709,
     "end_time": "2024-12-09T23:21:07.317272",
     "exception": false,
     "start_time": "2024-12-09T23:21:07.313563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries\n",
    "The first step involves importing the necessary libraries for data manipulation, model building, and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c9bf7a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:07.326889Z",
     "iopub.status.busy": "2024-12-09T23:21:07.326354Z",
     "iopub.status.idle": "2024-12-09T23:21:25.277081Z",
     "shell.execute_reply": "2024-12-09T23:21:25.275990Z"
    },
    "papermill": {
     "duration": 17.958835,
     "end_time": "2024-12-09T23:21:25.279988",
     "exception": false,
     "start_time": "2024-12-09T23:21:07.321153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             roc_curve, precision_recall_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05a70b",
   "metadata": {
    "papermill": {
     "duration": 0.003808,
     "end_time": "2024-12-09T23:21:25.288034",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.284226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extracting Valid Dataset\n",
    "A function is defined to generate a synthetic dataset with specified characteristics. This dataset simulates patient records with various features related to sepsis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66e77759",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:25.298171Z",
     "iopub.status.busy": "2024-12-09T23:21:25.297473Z",
     "iopub.status.idle": "2024-12-09T23:21:25.310892Z",
     "shell.execute_reply": "2024-12-09T23:21:25.309626Z"
    },
    "papermill": {
     "duration": 0.021132,
     "end_time": "2024-12-09T23:21:25.313325",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.292193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_sepsis_dataset(n_samples=2000):\n",
    "    categorical_features = {\n",
    "        'sex': np.random.choice(['male', 'female'], n_samples),\n",
    "        'ethnicity': np.random.choice(['white', 'black', 'hispanic', 'asian', 'other'], n_samples),\n",
    "        'metastatic_cancer': np.random.choice([0, 1], n_samples, p=[0.85, 0.15]),\n",
    "        'diabetes': np.random.choice([0, 1], n_samples, p=[0.70, 0.30])\n",
    "    }\n",
    "    quantitative_features = {\n",
    "        'age': np.random.normal(65, 15, n_samples),\n",
    "        'hospital_elixhauser': np.random.normal(4, 2, n_samples),\n",
    "        'vent': np.random.choice([0, 1], n_samples),\n",
    "        'couch': np.random.normal(1.5, 0.5, n_samples),\n",
    "        'sirs': np.random.normal(3, 1, n_samples),\n",
    "        'qsofa': np.random.normal(1, 0.5, n_samples),\n",
    "        'anion_gap_medium': np.random.normal(15, 4, n_samples),\n",
    "        'bocarbonate_medium': np.random.normal(24, 3, n_samples),\n",
    "        'creatinine_medium': np.random.normal(1.2, 0.4, n_samples),\n",
    "        'glucose_medium': np.random.normal(100, 25, n_samples),\n",
    "        'hemoglobin_medium': np.random.normal(12.5, 2, n_samples),\n",
    "        'lactate_medium': np.random.normal(2.5, 1, n_samples),\n",
    "        'platelet_means': np.random.normal(250, 50, n_samples),\n",
    "        'potassium_means': np.random.normal(4, 0.5, n_samples),\n",
    "        'inr_means': np.random.normal(1.1, 0.2, n_samples),\n",
    "        'sodium_means': np.random.normal(140, 3, n_samples),\n",
    "        'wbc_means': np.random.normal(10, 2.5, n_samples),\n",
    "        'heart_rate_means': np.random.normal(90, 15, n_samples),\n",
    "        'sys_bp_means': np.random.normal(120, 15, n_samples),\n",
    "        'dias_bp_means': np.random.normal(80, 10, n_samples),\n",
    "        'resp_rate_means': np.random.normal(18, 4, n_samples),\n",
    "        'temp_c_means': np.random.normal(37, 0.5, n_samples),\n",
    "        'spo2_medians': np.random.normal(95, 2, n_samples),\n",
    "        'urine_output': np.random.normal(1500, 500, n_samples)\n",
    "    }\n",
    "    outcomes = {\n",
    "        'sepsis': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),\n",
    "        'hospital_expire_flag': np.random.choice([0, 1], n_samples, p=[0.9, 0.1])\n",
    "    }\n",
    "    df = pd.DataFrame({**categorical_features, **quantitative_features, **outcomes})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc70005",
   "metadata": {
    "papermill": {
     "duration": 0.003791,
     "end_time": "2024-12-09T23:21:25.321166",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.317375",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Augmentation\n",
    "This function introduces missing values and addresses class imbalance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2faea417",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:25.332452Z",
     "iopub.status.busy": "2024-12-09T23:21:25.332038Z",
     "iopub.status.idle": "2024-12-09T23:21:25.339783Z",
     "shell.execute_reply": "2024-12-09T23:21:25.338581Z"
    },
    "papermill": {
     "duration": 0.016927,
     "end_time": "2024-12-09T23:21:25.342056",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.325129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def augment_data_with_missing_and_imbalance(df, missing_ratio=0.1, imbalance_ratio=0.3):\n",
    "    df_with_missing = df.copy()\n",
    "    n_missing = int(missing_ratio * df_with_missing.size)\n",
    "    random_indices = np.random.choice(df_with_missing.size, n_missing, replace=False)\n",
    "    flat_data = df_with_missing.to_numpy().flatten()\n",
    "    flat_data[random_indices] = np.nan\n",
    "    df_with_missing = pd.DataFrame(flat_data.reshape(df_with_missing.shape), columns=df_with_missing.columns)\n",
    "    minority_class = df_with_missing[df_with_missing['hospital_expire_flag'] == 1]\n",
    "    majority_class = df_with_missing[df_with_missing['hospital_expire_flag'] == 0]\n",
    "    target_majority_size = int(len(minority_class) / imbalance_ratio)\n",
    "    majority_class = majority_class.sample(target_majority_size, random_state=42, replace=True)\n",
    "    df_imbalanced = pd.concat([minority_class, majority_class]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return df_imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe97fd2",
   "metadata": {
    "papermill": {
     "duration": 0.004253,
     "end_time": "2024-12-09T23:21:25.350481",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.346228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing Pipeline\n",
    "The preprocessing pipeline handles missing values and prepares the data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a848dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:25.360290Z",
     "iopub.status.busy": "2024-12-09T23:21:25.359893Z",
     "iopub.status.idle": "2024-12-09T23:21:25.375125Z",
     "shell.execute_reply": "2024-12-09T23:21:25.373790Z"
    },
    "papermill": {
     "duration": 0.023346,
     "end_time": "2024-12-09T23:21:25.377750",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.354404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_important_features(X, y):\n",
    "    \"\"\"\n",
    "    Select the most important features using mutual information.\n",
    "    \"\"\"\n",
    "    mutual_info = mutual_info_classif(X, y)\n",
    "    feature_importances = pd.Series(mutual_info, index=X.columns)\n",
    "    selected_features = feature_importances[feature_importances > 0.01].index  # Adjust threshold as needed\n",
    "    if len(selected_features) == 0:  # If no features meet the threshold\n",
    "        return X  # Fallback to all features\n",
    "    return X[selected_features]\n",
    "\n",
    "def balance_data_with_smote(X, y):\n",
    "    \"\"\"\n",
    "    Balance the dataset using SMOTE with validation for numeric data.\n",
    "    \"\"\"\n",
    "    # Ensure that all features in X are numeric\n",
    "    if not np.issubdtype(X.dtypes, np.number):\n",
    "        X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Check for NaN or invalid values and fill them (fallback to median)\n",
    "    if X.isnull().any().any():\n",
    "        X = X.fillna(X.median())\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    try:\n",
    "        X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "    except ValueError as e:\n",
    "        print(f\"SMOTE Error: {e}. Falling back to simple oversampling.\")\n",
    "        # Fallback to naive oversampling\n",
    "        minority_class = X[y == 1]\n",
    "        majority_class = X[y == 0]\n",
    "        minority_oversampled = minority_class.sample(len(majority_class), replace=True, random_state=42)\n",
    "        X_balanced = pd.concat([majority_class, minority_oversampled])\n",
    "        y_balanced = np.concatenate([np.zeros(len(majority_class)), np.ones(len(minority_oversampled))])\n",
    "    return X_balanced, y_balanced\n",
    "\n",
    "\n",
    "def improved_preprocessing_pipeline(df):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline with feature selection and SMOTE for balancing.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=['hospital_expire_flag'], axis=1)\n",
    "    y = df['hospital_expire_flag'].astype(int)\n",
    "\n",
    "    # Handle missing values\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    numerical_cols = X.select_dtypes(include=['number', 'bool']).columns\n",
    "\n",
    "    if not categorical_cols.empty:\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        X_categorical = pd.DataFrame(cat_imputer.fit_transform(X[categorical_cols]), columns=categorical_cols)\n",
    "    else:\n",
    "        X_categorical = pd.DataFrame()\n",
    "\n",
    "    if not numerical_cols.empty:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        X_numerical = pd.DataFrame(num_imputer.fit_transform(X[numerical_cols]), columns=numerical_cols)\n",
    "    else:\n",
    "        X_numerical = pd.DataFrame()\n",
    "\n",
    "    if not X_categorical.empty:\n",
    "        X_categorical_encoded = pd.get_dummies(X_categorical, drop_first=True)\n",
    "    else:\n",
    "        X_categorical_encoded = pd.DataFrame()\n",
    "\n",
    "    # Combine numerical and encoded categorical features\n",
    "    X_processed = pd.concat([X_categorical_encoded, X_numerical], axis=1)\n",
    "\n",
    "    # Feature selection\n",
    "    X_selected = select_important_features(X_processed, y)\n",
    "\n",
    "    # Ensure X_selected is valid for SMOTE\n",
    "    X_selected = X_selected.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
    "    X_selected = X_selected.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    # Balance the dataset using SMOTE\n",
    "    X_balanced, y_balanced = balance_data_with_smote(X_selected, y)\n",
    "\n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_balanced)\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    return X_scaled, y_balanced, cv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774c3d7",
   "metadata": {
    "papermill": {
     "duration": 0.003661,
     "end_time": "2024-12-09T23:21:25.385639",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.381978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Definition with Attention Mechanism\n",
    "The model architecture includes dense layers and a custom attention layer to enhance feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4a54eaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:25.395899Z",
     "iopub.status.busy": "2024-12-09T23:21:25.394818Z",
     "iopub.status.idle": "2024-12-09T23:21:25.407407Z",
     "shell.execute_reply": "2024-12-09T23:21:25.406065Z"
    },
    "papermill": {
     "duration": 0.020154,
     "end_time": "2024-12-09T23:21:25.409717",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.389563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4: Model Definition\n",
    "class CustomAttentionLayer(Layer):\n",
    "    def __init__(self, heads=4, key_dim=32, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.heads = heads\n",
    "        self.key_dim = key_dim\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.query_dense = Dense(self.heads * self.key_dim)\n",
    "        self.key_dense = Dense(self.heads * self.key_dim)\n",
    "        self.value_dense = Dense(self.heads * self.key_dim)\n",
    "        self.output_dense = Dense(input_shape[-1])\n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        scores = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.cast(self.key_dim, tf.float32))\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        attended = tf.matmul(attention_weights, value)\n",
    "        return self.output_dense(attended)\n",
    "\n",
    "def create_advanced_mortality_model(input_shape):\n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    attention_output = CustomAttentionLayer(heads=8, key_dim=64)(x)\n",
    "    combined = x + attention_output\n",
    "    x = Dense(64, activation='relu')(combined)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a0c9d",
   "metadata": {
    "papermill": {
     "duration": 0.00358,
     "end_time": "2024-12-09T23:21:25.417274",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.413694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comprehensive Model Evaluation and Visualization Functions\n",
    "This section includes functions for evaluating the model's performance using cross-validation and plotting ROC and Precision-Recall curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef1b7a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:25.426553Z",
     "iopub.status.busy": "2024-12-09T23:21:25.426126Z",
     "iopub.status.idle": "2024-12-09T23:21:25.438577Z",
     "shell.execute_reply": "2024-12-09T23:21:25.437311Z"
    },
    "papermill": {
     "duration": 0.019705,
     "end_time": "2024-12-09T23:21:25.440755",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.421050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5: Comprehensive Evaluation\n",
    "def plot_roc_and_pr_curves(y_true, y_pred_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(fpr, tpr, label='ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(recall, precision, label='Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def comprehensive_model_evaluation(X, y, cv):\n",
    "    cv_scores = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'auc': []}\n",
    "    for train_idx, val_idx in cv.split(X, y):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        model = create_advanced_mortality_model(X_train.shape[1])\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=3)\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, callbacks=[early_stopping, lr_reducer], verbose=0)\n",
    "        y_pred_proba = model.predict(X_val).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        cv_scores['accuracy'].append(accuracy_score(y_val, y_pred))\n",
    "        cv_scores['precision'].append(precision_score(y_val, y_pred))\n",
    "        cv_scores['recall'].append(recall_score(y_val, y_pred))\n",
    "        cv_scores['f1'].append(f1_score(y_val, y_pred))\n",
    "        cv_scores['auc'].append(roc_auc_score(y_val, y_pred_proba))\n",
    "        plot_roc_and_pr_curves(y_val, y_pred_proba)\n",
    "    print(\"Cross-Validation Results:\")\n",
    "    for metric, scores in cv_scores.items():\n",
    "        print(f\"{metric.capitalize()}: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af25ea5",
   "metadata": {
    "papermill": {
     "duration": 0.003794,
     "end_time": "2024-12-09T23:21:25.448498",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.444704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main Execution Block\n",
    "Finally, the main execution block ties all components together to run the entire workflow from data generation through evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76ac794d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-09T23:21:25.457882Z",
     "iopub.status.busy": "2024-12-09T23:21:25.457514Z",
     "iopub.status.idle": "2024-12-09T23:23:50.143877Z",
     "shell.execute_reply": "2024-12-09T23:23:50.142518Z"
    },
    "papermill": {
     "duration": 144.694715,
     "end_time": "2024-12-09T23:23:50.147192",
     "exception": false,
     "start_time": "2024-12-09T23:21:25.452477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6592/1427360988.py:73: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X_selected = X_selected.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
      "I0000 00:00:1745328349.241784    6592 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22272 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1745328351.192032    7583 service.cc:152] XLA service 0x7c9218002e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1745328351.192049    7583 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2025-04-22 19:10:51.233668: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "E0000 00:00:1745328351.484504    7583 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "E0000 00:00:1745328351.587138    7583 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2025-04-22 19:10:51.595765: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at xla_ops.cc:591 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "2025-04-22 19:10:51.595798: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/sunway/miniconda3/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/home/sunway/miniconda3/lib/python3.12/runpy.py\", line 88, in _run_code\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/sunway/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/home/sunway/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/home/sunway/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n\n  File \"/tmp/ipykernel_6592/3749745590.py\", line 5, in <module>\n\n  File \"/tmp/ipykernel_6592/2819076226.py\", line 31, in comprehensive_model_evaluation\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_5622]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFailedPreconditionError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m df_augmented = augment_data_with_missing_and_imbalance(df, missing_ratio=\u001b[32m0.1\u001b[39m, imbalance_ratio=\u001b[32m0.3\u001b[39m)\n\u001b[32m      4\u001b[39m X, y, cv = improved_preprocessing_pipeline(df_augmented)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mcomprehensive_model_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mcomprehensive_model_evaluation\u001b[39m\u001b[34m(X, y, cv)\u001b[39m\n\u001b[32m     29\u001b[39m early_stopping = EarlyStopping(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, patience=\u001b[32m10\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     30\u001b[39m lr_reducer = ReduceLROnPlateau(monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m, factor=\u001b[32m0.7\u001b[39m, patience=\u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_reducer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m y_pred_proba = model.predict(X_val).flatten()\n\u001b[32m     33\u001b[39m y_pred = (y_pred_proba > \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mFailedPreconditionError\u001b[39m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/sunway/miniconda3/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/home/sunway/miniconda3/lib/python3.12/runpy.py\", line 88, in _run_code\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/sunway/miniconda3/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/home/sunway/miniconda3/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/home/sunway/miniconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3047, in run_cell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3102, in _run_cell\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3489, in run_ast_nodes\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3549, in run_code\n\n  File \"/tmp/ipykernel_6592/3749745590.py\", line 5, in <module>\n\n  File \"/tmp/ipykernel_6592/2819076226.py\", line 31, in comprehensive_model_evaluation\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/home/sunway/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_5622]"
     ]
    }
   ],
   "source": [
    "# Step 6: Main Execution\n",
    "df = generate_synthetic_sepsis_dataset()\n",
    "df_augmented = augment_data_with_missing_and_imbalance(df, missing_ratio=0.1, imbalance_ratio=0.3)\n",
    "X, y, cv = improved_preprocessing_pipeline(df_augmented)\n",
    "comprehensive_model_evaluation(X, y, cv)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2061039,
     "sourceId": 3419588,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 171.025364,
   "end_time": "2024-12-09T23:23:53.655660",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-09T23:21:02.630296",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
